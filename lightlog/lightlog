#!/usr/bin/env python
from __future__ import print_function
from argparse import ArgumentParser
import os, json, errno, subprocess
from os.path import expanduser, isdir, join, dirname
from sys import exit
from klein import Klein, run, route
from twisted.internet.task import LoopingCall
from twisted.internet import defer, task, reactor, threads
from twisted.python import log
from twisted.internet.defer import inlineCallbacks, returnValue
from datetime import datetime
from time import sleep

store_prefix = None
after_dump_cmd = None
cmd_timeout = None
write_buffer = {}

def jsonize(d, req=None):
  if req:
    req.setHeader("Content-Type", "application/json")
  return json.dumps(d, default=lambda o: o.isoformat()+"Z" if isinstance(o, datetime) else o.__dict__)

def mkdir_p(path):
  try:
    os.makedirs(path)
  except OSError as e:
    if e.errno == errno.EEXIST and isdir(path):
      pass
    else:
      raise

def shard_name(thing, year, month, day):
  return join(store_prefix, thing, "%04d/%02d/%02d.json" % (year, month, day))

@route("/write/<thing>", methods=["POST"])
def write(req, thing):
  global write_buffer
  try:
    buf = { k:float(v[0]) for (k,v) in req.args.iteritems() }
    buf["timestamp"] = datetime.utcnow()
  except ValueError:
    req.setResponseCode(400)
    return jsonize({"error": "only floats supported"}, req)
  key = shard_name(thing, buf["timestamp"].year, buf["timestamp"].month, buf["timestamp"].day)
  write_buffer[key] = write_buffer.get(key, []) + [buf]
  return jsonize(buf, req)

@route("/read/<thing>/<year>/<month>/<day>.json", methods=["GET"])
@inlineCallbacks
def read(req, thing, year, month, day):
  req.setHeader("Access-Control-Allow-Origin", "*")
  try:
    year = int(year)
    month = int(month)
    day = int(day)
  except ValueError:
    req.setResponseCode(400)
    errmsg = yield {"error": "invalid numbers in date"}
    returnValue(jsonize(errmsg, req))
  data = yield threads.deferToThread(async_read, thing, year, month, day)
  if not data:
    req.setResponseCode(404)
  returnValue(jsonize(data, req))

def async_read(thing, year, month, day):
  try:
    fn = shard_name(thing, year, month, day)
    with open(fn) as fp:
      data = json.loads(fp.read())
  except IOError:
    log.msg("shard %s not found, returning empty data" % fn)
    return []
  except ValueError:
    log.msg("JSON data is corrupted on shard %s" % fn)
    return []
  if fn in write_buffer:
    data = data + write_buffer[fn]
  return data

@inlineCallbacks
def dump_buf():
  global write_buffer
  if not write_buffer:
    return
  wb = write_buffer.copy()
  write_buffer = {}
  yield threads.deferToThread(async_dump_buf, wb)

def async_dump_buf(wb):
  for sh,content in wb.items():
    content.sort(key=lambda x: x["timestamp"])
    try:
      with open(sh) as fp:
        content = json.loads(fp.read()) + content
    except:
      log.msg("cannot read shard %s from store, creating" % sh)
    mkdir_p(dirname(sh))
    with open(sh+".0", "w") as fp:
      fp.write(jsonize(content))
    os.rename(sh+".0", sh)  # atomic
  run_after_dump_cmd()

def run_after_dump_cmd():
  if not after_dump_cmd:
    return 0
  cmd = after_dump_cmd.format(store_prefix=store_prefix)
  log.msg("running after dump command")
  popen = subprocess.Popen([ "timeout", "-s", "9", str(cmd_timeout), "bash", "-c", cmd ],
                           shell=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
  out = popen.communicate()[0]
  log.msg("after dump command returned %d" % popen.returncode)
  if popen.returncode != 0:
    for line in out.split("\n"):
      log.msg("error %d: %s" % (popen.returncode, line))
  return popen.returncode

@inlineCallbacks
def schedule_init_dump(every):
  log.msg("executing initial after dump command")
  rv = yield threads.deferToThread(run_after_dump_cmd)
  log.msg("scheduling dump every %d s" % every)
  dump_task = task.LoopingCall(dump_buf)
  dump_task.start(every)

if __name__ == "__main__":
  parser = ArgumentParser()
  parser.add_argument("--store", dest="store", default="~/.lightlog",
                      help="Where is the datastore")
  parser.add_argument("--after-dump-cmd", dest="after_dump_cmd", default="",
                      help="Command to execute after writing data")
  parser.add_argument("--host", dest="host", default="localhost",
                      help="Listen on this host")
  parser.add_argument("--port", dest="port", default=4242, type=int,
                      help="Listen on this port")
  parser.add_argument("--dump-every", dest="sync_every", default=300, type=int,
                      help="Dump data every SYNC_EVERY seconds")
  parser.add_argument("--cmd-timeout", dest="cmd_timeout", default=60, type=int,
                      help="Kill after dump command after CMD_TIMEOUT seconds")
  args = parser.parse_args()
  store_prefix = expanduser(args.store)
  after_dump_cmd = args.after_dump_cmd
  cmd_timeout = args.cmd_timeout
  reactor.callLater(2, schedule_init_dump, args.sync_every)
  run(args.host, args.port)
  exit(0)
